# AI-Memory-Assistant
This project is an AI-powered Question Answering web application built using a Retrieval-Augmented Generation (RAG) architecture that combines semantic search with a locally hosted Large Language Model to generate accurate and context-aware responses. The system is developed using Flask as the backend framework to handle routing, API requests, and server-side logic, while the frontend is built using HTML, CSS, and JavaScript to provide an interactive user interface. The core intelligence of the system is powered by a locally running Large Language Model served through Ollama, specifically using the Mistral model for generating natural language responses. To enable efficient semantic understanding of text, the project uses SentenceTransformers to convert document content and user queries into dense vector embeddings. These embeddings are stored and indexed using FAISS, which performs high-speed similarity search to retrieve the most relevant document chunks based on the userâ€™s question. The workflow begins by processing and embedding textual data, storing it in the FAISS vector database, and then, when a user submits a query, the system retrieves the most semantically relevant content. This retrieved context is appended to a structured prompt and sent to the Mistral model via Ollama, which generates a context-aware answer. The final response is returned through Flask as a JSON API and displayed on the web interface. The project demonstrates the integration of vector databases, embedding models, and large language models into a complete end-to-end AI system that runs entirely on a local machine without relying on external cloud APIs, showcasing practical implementation of modern NLP concepts such as embeddings, semantic search, and retrieval-augmented generation.
